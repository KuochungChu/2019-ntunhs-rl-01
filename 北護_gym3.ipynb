{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3. 練習 OpenAI gym \n",
    "\n",
    "大家好! 歡迎各位回來增強式學習的世界\n",
    "\n",
    "* 因為我們知道選擇action的方式決定了遊戲分數，因此我們嘗試比較多種選擇的方式\n",
    "* action: 隨機、指定、ε-greedy 的 policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這是隨機的分數: 20.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "\n",
    "#預設為0\n",
    "observation = env.reset()\n",
    "#用來在colab上畫圖的指令\n",
    "prev_screen = env.render(mode='rgb_array')\n",
    "plt.imshow(prev_screen)\n",
    "\n",
    "rewards = 0\n",
    "for t in range(20):\n",
    "  screen = env.render(mode='rgb_array')\n",
    "  action = env.action_space.sample()\n",
    "\n",
    "  observation, reward, done, info = env.step(action)\n",
    "    \n",
    "  rewards+=reward  \n",
    "  \n",
    "  #用matploblib畫出來\n",
    "  plt.imshow(screen)\n",
    "  display.clear_output(wait=True)\n",
    "  display.display(plt.gcf())\n",
    "  \n",
    "  if done==True:\n",
    "    break\n",
    "      \n",
    "display.clear_output(wait=True)\n",
    "env.close()\n",
    "print(\"這是隨機的分數:\",rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 首先你可以注意到，我們在選擇action的時候是用亂數選。這樣的選擇，得到的分數是20.0分\n",
    "2. 我們要以能夠突破這個分數為目標前進！\n",
    "\n",
    "# Space\n",
    "\n",
    "在 Gym 的環境中，有兩種空間：action_space 和observation_space。如果我們看一下類型的話，兩者都屬於Space類型，用來描述有效的動作還有環境的格式和範圍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "#檢視一下，關於遊戲的基本資訊\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "print(env.action_space)\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 環境狀態 (Observation)\n",
    "\n",
    "在gym中，observation就是我們在課堂上提到的state，也就是指環境裡面的狀態。\n",
    "\n",
    "在上面的程式中，我們可以看到env.step()提供環境狀態。env.step() 會回傳 4 個變數，分別是\n",
    "1. observation (環境狀態)\n",
    "2. reward (上一次 action 獲得的 reward )\n",
    "3. done (判斷是否達到終止條件的變數)\n",
    "4. info ( debug 用的資訊)\n",
    "\n",
    "呼叫 reset，整個環境就會重頭開始，此外 reset 會回傳一個初始的環境狀態。\n",
    "\n",
    "基本上，我們可以看到action_space 是一個Discrete類別的物件，他是一個{0,1,...,n-1}的非負整數的集合。在 CartPole-v0 例子中，動作空間為 {0,1}，代表往左往右。observation_space 則是一個Box類型的物件，用來表示一个 n 维的盒子，所以在上面我們會發現 observation 是一個長度4的數組，每個數組都有上下界（看起來就像一個盒子一般)。\n",
    "\n",
    "再次提醒，如果需要知道詳細的action以及observation所代表的意義請查這邊：https://github.com/openai/gym/wiki/CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果一直讓滑車往左的話，一下子遊戲就停掉了，這次的分數: 9.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAARuElEQVR4nO3dcazdZ33f8fdnTgisoCYhN5FnO3NaXI1QFYfdBVfZH2mgbYiqGSSoklVgoUimUpBAQtsSJg2QFqmVVlKhbRGukmEmRsgKKFaUlbomqOIPEm7AGBuTxoBFbm3Fl5EEEFq2hO/+OM8lp86xfXzvPbn3uef9kn76/X7P7znnfB/l5OOfn/Mcn1QVkqR+/KPVLkCSdH4MbknqjMEtSZ0xuCWpMwa3JHXG4JakzkwsuJPcmOTxJMeS3D6p15GkaZNJrONOsgH4O+B3gXng68AtVfWdFX8xSZoyk7rjvhY4VlXfr6r/C9wH7JzQa0nSVLlgQs+7CXhy6HweePOZOl922WW1devWCZUiSf05fvw4P/rRjzLq2qSCe9SL/YM5mSS7gd0AV155JXNzcxMqRZL6Mzs7e8Zrk5oqmQe2DJ1vBk4Md6iqPVU1W1WzMzMzEypDktafSQX314FtSa5K8grgZmDfhF5LkqbKRKZKqur5JO8HvgRsAO6tqiOTeC1JmjaTmuOmqh4CHprU80vStPKbk5LUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOrOsny5Lchz4KfAC8HxVzSa5FPgcsBU4DvxhVT29vDIlSYtW4o77d6pqe1XNtvPbgQNVtQ040M4lSStkElMlO4G97Xgv8PYJvIYkTa3lBncBf53ksSS7W9sVVXUSoO0vX+ZrSJKGLGuOG7iuqk4kuRzYn+S74z6wBf1ugCuvvHKZZUjS9FjWHXdVnWj7U8AXgWuBp5JsBGj7U2d47J6qmq2q2ZmZmeWUIUlTZcnBneRXkrxm8Rj4PeAwsA/Y1brtAh5YbpGSpBctZ6rkCuCLSRaf539U1V8l+Tpwf5JbgR8C71p+mZKkRUsO7qr6PvDGEe3/G3jLcoqSJJ2Z35yUpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOnPO4E5yb5JTSQ4PtV2aZH+SJ9r+ktaeJJ9IcizJoSRvmmTxkjSNxrnj/hRw42lttwMHqmobcKCdA7wN2Na23cDdK1OmJGnROYO7qv4W+PFpzTuBve14L/D2ofZP18DXgIuTbFypYiVJS5/jvqKqTgK0/eWtfRPw5FC/+db2Ekl2J5lLMrewsLDEMiRp+qz0h5MZ0VajOlbVnqqararZmZmZFS5DktavpQb3U4tTIG1/qrXPA1uG+m0GTiy9PEnS6ZYa3PuAXe14F/DAUPt72uqSHcCzi1MqkqSVccG5OiT5LHA9cFmSeeAjwJ8A9ye5Ffgh8K7W/SHgJuAY8HPgvROoWZKm2jmDu6puOcOlt4zoW8Btyy1KknRmfnNSkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnzhncSe5NcirJ4aG2jyb5+yQH23bT0LU7khxL8niS359U4ZI0rca54/4UcOOI9ruqanvbHgJIcjVwM/CG9pj/mmTDShUrSRojuKvqb4Efj/l8O4H7quq5qvoBg197v3YZ9UmSTrOcOe73JznUplIuaW2bgCeH+sy3tpdIsjvJXJK5hYWFZZQhSdNlqcF9N/DrwHbgJPBnrT0j+taoJ6iqPVU1W1WzMzMzSyxDkqbPkoK7qp6qqheq6hfAX/DidMg8sGWo62bgxPJKlCQNW1JwJ9k4dPoOYHHFyT7g5iQXJbkK2AY8urwSJUnDLjhXhySfBa4HLksyD3wEuD7JdgbTIMeB9wFU1ZEk9wPfAZ4HbquqFyZTuiRNp3MGd1XdMqL5nrP0vxO4czlFSZLOzG9OSlJnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUmXOu45am1WN73veStn+++5OrUIn0D3nHLZ2HUWEuvdwMbknqjMEtnQenSrQWGNyS1BmDWxrBuWytZQa3JHXG4JakzhjcktQZg1sakytKtFYY3JLUmXMGd5ItSR5OcjTJkSQfaO2XJtmf5Im2v6S1J8knkhxLcijJmyY9CGkl+VV3rXXj3HE/D3yoql4P7ABuS3I1cDtwoKq2AQfaOcDbGPy6+zZgN3D3ilctSVPsnMFdVSer6hvt+KfAUWATsBPY27rtBd7ejncCn66BrwEXJ9m44pVLE+D6bfXgvOa4k2wFrgEeAa6oqpMwCHfg8tZtE/Dk0MPmW9vpz7U7yVySuYWFhfOvXJKm1NjBneTVwOeBD1bVT87WdURbvaShak9VzVbV7MzMzLhlSNLUGyu4k1zIILQ/U1VfaM1PLU6BtP2p1j4PbBl6+GbgxMqUK0kaZ1VJgHuAo1X18aFL+4Bd7XgX8MBQ+3va6pIdwLOLUypSj1xRorVmnF/AuQ54N/DtJAdb24eBPwHuT3Ir8EPgXe3aQ8BNwDHg58B7V7RiSZpy5wzuqvoqo+etAd4yon8Bty2zLull54oS9cJvTkpSZwxuSeqMwS2dhR9Mai0yuCWpMwa3hB9Mqi8GtyR1xuCWpM4Y3JLUGYNbOgNXlGitMrglqTMGt6aeK0rUG4NbkjpjcEtSZwxuSeqMwS2N4IoSrWUGtyR1xuDWVHNFiXpkcEtSZ8b5seAtSR5OcjTJkSQfaO0fTfL3SQ627aahx9yR5FiSx5P8/iQHIEnTZpwfC34e+FBVfSPJa4DHkuxv1+6qqv803DnJ1cDNwBuAfwL8TZLfqKoXVrJwaVL8YFJr3TnvuKvqZFV9ox3/FDgKbDrLQ3YC91XVc1X1Awa/9n7tShQrSTrPOe4kW4FrgEda0/uTHEpyb5JLWtsm4Mmhh81z9qCXVoUfTKpXYwd3klcDnwc+WFU/Ae4Gfh3YDpwE/myx64iH14jn251kLsncwsLCeRcuTYLTJOrBWMGd5EIGof2ZqvoCQFU9VVUvVNUvgL/gxemQeWDL0MM3AydOf86q2lNVs1U1OzMzs5wxSNJUGWdVSYB7gKNV9fGh9o1D3d4BHG7H+4Cbk1yU5CpgG/DoypUsSdNtnFUl1wHvBr6d5GBr+zBwS5LtDKZBjgPvA6iqI0nuB77DYEXKba4okaSVc87grqqvMnre+qGzPOZO4M5l1CVJOgO/OampNGpFiR9MqhcGtyR1xuDW1HH9tnpncEtSZwxuSeqMwS1JnTG4JVxRor4Y3JLUGYNbU8UVJVoPDG5J6ozBLUmdMbglqTMGt6aeK0rUG4NbXUtyXttynkNaKwxuTY25T+5e7RKkFTHODylI68aDJ18M7z/YuGcVK5GWzjtuTY3h0B51LvXC4NZUm32fd93qzzg/FvzKJI8m+VaSI0k+1tqvSvJIkieSfC7JK1r7Re38WLu+dbJDkKTpMs4d93PADVX1RmA7cGOSHcCfAndV1TbgaeDW1v9W4Omqeh1wV+snrbrT57Sd41avxvmx4AJ+1k4vbFsBNwD/urXvBT4K3A3sbMcAfwn85yRpzyOtmsG0yIth/dFVq0RanrHmuJNsSHIQOAXsB74HPFNVz7cu88CmdrwJeBKgXX8WeO1KFi1J02ys4K6qF6pqO7AZuBZ4/ahubT/qmwovudtOsjvJXJK5hYWFceuVpKl3XqtKquoZ4CvADuDiJItTLZuBE+14HtgC0K7/KvDjEc+1p6pmq2p2ZmZmadVL0hQaZ1XJTJKL2/GrgLcCR4GHgXe2bruAB9rxvnZOu/5l57claeWM883JjcDeJBsYBP39VfVgku8A9yX5j8A3gXta/3uA/57kGIM77ZsnULckTa1xVpUcAq4Z0f59BvPdp7f/H+BdK1KdJOkl/OakJHXG4JakzhjcktQZ/1lXdc0FS5pG3nFLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM6M82PBr0zyaJJvJTmS5GOt/VNJfpDkYNu2t/Yk+USSY0kOJXnTpAchSdNknH+P+znghqr6WZILga8m+V/t2r+pqr88rf/bgG1tezNwd9tLklbAOe+4a+Bn7fTCtp3tX6/fCXy6Pe5rwMVJNi6/VEkSjDnHnWRDkoPAKWB/VT3SLt3ZpkPuSnJRa9sEPDn08PnWJklaAWMFd1W9UFXbgc3AtUl+E7gD+GfAvwAuBf5d655RT3F6Q5LdSeaSzC0sLCypeEmaRue1qqSqngG+AtxYVSfbdMhzwH8Drm3d5oEtQw/bDJwY8Vx7qmq2qmZnZmaWVLwkTaNxVpXMJLm4Hb8KeCvw3cV56yQB3g4cbg/ZB7ynrS7ZATxbVScnUr0kTaFxVpVsBPYm2cAg6O+vqgeTfDnJDIOpkYPAH7f+DwE3AceAnwPvXfmyJWl6nTO4q+oQcM2I9hvO0L+A25ZfmiRpFL85KUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOpOqWu0aSPJT4PHVrmNCLgN+tNpFTMB6HRes37E5rr7806qaGXXhgpe7kjN4vKpmV7uISUgytx7Htl7HBet3bI5r/XCqRJI6Y3BLUmfWSnDvWe0CJmi9jm29jgvW79gc1zqxJj6clCSNb63ccUuSxrTqwZ3kxiSPJzmW5PbVrud8Jbk3yakkh4faLk2yP8kTbX9Ja0+ST7SxHkryptWr/OySbEnycJKjSY4k+UBr73psSV6Z5NEk32rj+lhrvyrJI21cn0vyitZ+UTs/1q5vXc36zyXJhiTfTPJgO18v4zqe5NtJDiaZa21dvxeXY1WDO8kG4L8AbwOuBm5JcvVq1rQEnwJuPK3tduBAVW0DDrRzGIxzW9t2A3e/TDUuxfPAh6rq9cAO4Lb236b3sT0H3FBVbwS2Azcm2QH8KXBXG9fTwK2t/63A01X1OuCu1m8t+wBwdOh8vYwL4HeqavvQ0r/e34tLV1WrtgG/DXxp6PwO4I7VrGmJ49gKHB46fxzY2I43MlinDvBJ4JZR/db6BjwA/O56Ghvwj4FvAG9m8AWOC1r7L9+XwJeA327HF7R+We3azzCezQwC7AbgQSDrYVytxuPAZae1rZv34vluqz1Vsgl4cuh8vrX17oqqOgnQ9pe39i7H2/4afQ3wCOtgbG064SBwCtgPfA94pqqeb12Ga//luNr1Z4HXvrwVj+3PgX8L/KKdv5b1MS6AAv46yWNJdre27t+LS7Xa35zMiLb1vMylu/EmeTXweeCDVfWTZNQQBl1HtK3JsVXVC8D2JBcDXwReP6pb23cxriR/AJyqqseSXL/YPKJrV+Macl1VnUhyObA/yXfP0re3sZ231b7jnge2DJ1vBk6sUi0r6akkGwHa/lRr72q8SS5kENqfqaovtOZ1MTaAqnoG+AqDOfyLkyzeyAzX/stxteu/Cvz45a10LNcB/yrJceA+BtMlf07/4wKgqk60/SkGf9heyzp6L56v1Q7urwPb2iffrwBuBvatck0rYR+wqx3vYjA/vNj+nvap9w7g2cW/6q01Gdxa3wMcraqPD13qemxJZtqdNkleBbyVwYd5DwPvbN1OH9fieN8JfLnaxOlaUlV3VNXmqtrK4P+jL1fVH9H5uACS/EqS1yweA78HHKbz9+KyrPYkO3AT8HcM5hn//WrXs4T6PwucBP4fgz/pb2UwV3gAeKLtL219w2AVzfeAbwOzq13/Wcb1Lxn89fIQcLBtN/U+NuC3gG+2cR0G/kNr/zXgUeAY8D+Bi1r7K9v5sXb911Z7DGOM8XrgwfUyrjaGb7XtyGJO9P5eXM7mNyclqTOrPVUiSTpPBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ35/18bEOHKwdssAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gym import spaces\n",
    "  \n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "#故意將action只設一個值\n",
    "env.action_space = spaces.Discrete(1)\n",
    "  \n",
    "observation = env.reset()\n",
    "#用來在colab上畫圖的指令\n",
    "prev_screen = env.render(mode='rgb_array')\n",
    "plt.imshow(prev_screen)\n",
    "\n",
    "rewards = 0\n",
    "for t in range(20):\n",
    "  screen = env.render(mode='rgb_array')\n",
    "  action = env.action_space.sample()\n",
    "  observation, reward, done, info = env.step(action)\n",
    "    \n",
    "  rewards+=reward  \n",
    "  \n",
    "  #用matploblib畫出來\n",
    "  plt.imshow(screen)\n",
    "  display.clear_output(wait=True)\n",
    "  display.display(plt.gcf())\n",
    "  \n",
    "  if done==True:\n",
    "    break\n",
    "      \n",
    "display.clear_output(wait=True)\n",
    "env.close()\n",
    "print(\"如果一直讓滑車往左的話，一下子遊戲就停掉了，這次的分數:\",rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 來個工人智慧\n",
    "\n",
    "為了讓agent不會走得太無腦，再來引進一個簡單的策略：如果柱子向左傾（角度 < 0），則小車左移以維持平衡，否則右移。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 65 flames, total rewards 65.0\n",
      "Episode finished after 36 flames, total rewards 36.0\n",
      "Episode finished after 25 flames, total rewards 25.0\n",
      "Episode finished after 25 flames, total rewards 25.0\n",
      "Episode finished after 26 flames, total rewards 26.0\n",
      "Episode finished after 46 flames, total rewards 46.0\n",
      "Episode finished after 51 flames, total rewards 51.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 41 flames, total rewards 41.0\n",
      "Episode finished after 35 flames, total rewards 35.0\n",
      "Episode finished after 36 flames, total rewards 36.0\n",
      "Episode finished after 48 flames, total rewards 48.0\n",
      "Episode finished after 42 flames, total rewards 42.0\n",
      "Episode finished after 41 flames, total rewards 41.0\n",
      "Episode finished after 25 flames, total rewards 25.0\n",
      "Episode finished after 46 flames, total rewards 46.0\n",
      "Episode finished after 35 flames, total rewards 35.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 47 flames, total rewards 47.0\n",
      "Episode finished after 45 flames, total rewards 45.0\n",
      "Episode finished after 46 flames, total rewards 46.0\n",
      "Episode finished after 42 flames, total rewards 42.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 52 flames, total rewards 52.0\n",
      "Episode finished after 47 flames, total rewards 47.0\n",
      "Episode finished after 66 flames, total rewards 66.0\n",
      "Episode finished after 45 flames, total rewards 45.0\n",
      "Episode finished after 35 flames, total rewards 35.0\n",
      "Episode finished after 31 flames, total rewards 31.0\n",
      "Episode finished after 42 flames, total rewards 42.0\n",
      "Episode finished after 34 flames, total rewards 34.0\n",
      "Episode finished after 46 flames, total rewards 46.0\n",
      "Episode finished after 68 flames, total rewards 68.0\n",
      "Episode finished after 38 flames, total rewards 38.0\n",
      "Episode finished after 25 flames, total rewards 25.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 42 flames, total rewards 42.0\n",
      "Episode finished after 55 flames, total rewards 55.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 46 flames, total rewards 46.0\n",
      "Episode finished after 26 flames, total rewards 26.0\n",
      "Episode finished after 48 flames, total rewards 48.0\n",
      "Episode finished after 49 flames, total rewards 49.0\n",
      "Episode finished after 32 flames, total rewards 32.0\n",
      "Episode finished after 25 flames, total rewards 25.0\n",
      "Episode finished after 25 flames, total rewards 25.0\n",
      "Episode finished after 62 flames, total rewards 62.0\n",
      "Episode finished after 26 flames, total rewards 26.0\n",
      "Episode finished after 45 flames, total rewards 45.0\n",
      "Episode finished after 41 flames, total rewards 41.0\n",
      "Episode finished after 51 flames, total rewards 51.0\n",
      "Episode finished after 52 flames, total rewards 52.0\n",
      "Episode finished after 37 flames, total rewards 37.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 36 flames, total rewards 36.0\n",
      "Episode finished after 38 flames, total rewards 38.0\n",
      "Episode finished after 35 flames, total rewards 35.0\n",
      "Episode finished after 36 flames, total rewards 36.0\n",
      "Episode finished after 38 flames, total rewards 38.0\n",
      "Episode finished after 52 flames, total rewards 52.0\n",
      "Episode finished after 34 flames, total rewards 34.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 53 flames, total rewards 53.0\n",
      "Episode finished after 37 flames, total rewards 37.0\n",
      "Episode finished after 51 flames, total rewards 51.0\n",
      "Episode finished after 31 flames, total rewards 31.0\n",
      "Episode finished after 26 flames, total rewards 26.0\n",
      "Episode finished after 56 flames, total rewards 56.0\n",
      "Episode finished after 31 flames, total rewards 31.0\n",
      "Episode finished after 34 flames, total rewards 34.0\n",
      "Episode finished after 46 flames, total rewards 46.0\n",
      "Episode finished after 36 flames, total rewards 36.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 55 flames, total rewards 55.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 24 flames, total rewards 24.0\n",
      "Episode finished after 52 flames, total rewards 52.0\n",
      "Episode finished after 56 flames, total rewards 56.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 51 flames, total rewards 51.0\n",
      "Episode finished after 49 flames, total rewards 49.0\n",
      "Episode finished after 47 flames, total rewards 47.0\n",
      "Episode finished after 51 flames, total rewards 51.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 47 flames, total rewards 47.0\n",
      "Episode finished after 31 flames, total rewards 31.0\n",
      "Episode finished after 52 flames, total rewards 52.0\n",
      "Episode finished after 52 flames, total rewards 52.0\n",
      "Episode finished after 47 flames, total rewards 47.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 32 flames, total rewards 32.0\n",
      "Episode finished after 35 flames, total rewards 35.0\n",
      "Episode finished after 34 flames, total rewards 34.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 51 flames, total rewards 51.0\n",
      "Episode finished after 45 flames, total rewards 45.0\n",
      "Episode finished after 34 flames, total rewards 34.0\n",
      "Episode finished after 51 flames, total rewards 51.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 45 flames, total rewards 45.0\n",
      "Episode finished after 35 flames, total rewards 35.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 42 flames, total rewards 42.0\n",
      "Episode finished after 55 flames, total rewards 55.0\n",
      "Episode finished after 46 flames, total rewards 46.0\n",
      "Episode finished after 55 flames, total rewards 55.0\n",
      "Episode finished after 49 flames, total rewards 49.0\n",
      "Episode finished after 47 flames, total rewards 47.0\n",
      "Episode finished after 38 flames, total rewards 38.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 35 flames, total rewards 35.0\n",
      "Episode finished after 49 flames, total rewards 49.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 59 flames, total rewards 59.0\n",
      "Episode finished after 37 flames, total rewards 37.0\n",
      "Episode finished after 31 flames, total rewards 31.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 26 flames, total rewards 26.0\n",
      "Episode finished after 51 flames, total rewards 51.0\n",
      "Episode finished after 31 flames, total rewards 31.0\n",
      "Episode finished after 54 flames, total rewards 54.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 50 flames, total rewards 50.0\n",
      "Episode finished after 43 flames, total rewards 43.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 37 flames, total rewards 37.0\n",
      "Episode finished after 26 flames, total rewards 26.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 42 flames, total rewards 42.0\n",
      "Episode finished after 47 flames, total rewards 47.0\n",
      "Episode finished after 38 flames, total rewards 38.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 34 flames, total rewards 34.0\n",
      "Episode finished after 35 flames, total rewards 35.0\n",
      "Episode finished after 25 flames, total rewards 25.0\n",
      "Episode finished after 25 flames, total rewards 25.0\n",
      "Episode finished after 46 flames, total rewards 46.0\n",
      "Episode finished after 48 flames, total rewards 48.0\n",
      "Episode finished after 55 flames, total rewards 55.0\n",
      "Episode finished after 26 flames, total rewards 26.0\n",
      "Episode finished after 31 flames, total rewards 31.0\n",
      "Episode finished after 32 flames, total rewards 32.0\n",
      "Episode finished after 38 flames, total rewards 38.0\n",
      "Episode finished after 35 flames, total rewards 35.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 49 flames, total rewards 49.0\n",
      "Episode finished after 33 flames, total rewards 33.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 57 flames, total rewards 57.0\n",
      "Episode finished after 25 flames, total rewards 25.0\n",
      "Episode finished after 38 flames, total rewards 38.0\n",
      "Episode finished after 47 flames, total rewards 47.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 45 flames, total rewards 45.0\n",
      "Episode finished after 38 flames, total rewards 38.0\n",
      "Episode finished after 57 flames, total rewards 57.0\n",
      "Episode finished after 37 flames, total rewards 37.0\n",
      "Episode finished after 57 flames, total rewards 57.0\n",
      "Episode finished after 58 flames, total rewards 58.0\n",
      "Episode finished after 41 flames, total rewards 41.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 60 flames, total rewards 60.0\n",
      "Episode finished after 48 flames, total rewards 48.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 25 flames, total rewards 25.0\n",
      "Episode finished after 34 flames, total rewards 34.0\n",
      "Episode finished after 37 flames, total rewards 37.0\n",
      "Episode finished after 46 flames, total rewards 46.0\n",
      "Episode finished after 38 flames, total rewards 38.0\n",
      "Episode finished after 34 flames, total rewards 34.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 37 flames, total rewards 37.0\n",
      "Episode finished after 40 flames, total rewards 40.0\n",
      "Episode finished after 39 flames, total rewards 39.0\n",
      "Episode finished after 55 flames, total rewards 55.0\n",
      "Episode finished after 35 flames, total rewards 35.0\n",
      "Episode finished after 45 flames, total rewards 45.0\n",
      "Episode finished after 52 flames, total rewards 52.0\n",
      "Episode finished after 41 flames, total rewards 41.0\n",
      "Episode finished after 52 flames, total rewards 52.0\n",
      "Episode finished after 51 flames, total rewards 51.0\n",
      "Episode finished after 51 flames, total rewards 51.0\n",
      "Episode finished after 45 flames, total rewards 45.0\n",
      "Episode finished after 56 flames, total rewards 56.0\n",
      "Episode finished after 34 flames, total rewards 34.0\n",
      "Episode finished after 25 flames, total rewards 25.0\n",
      "Episode finished after 32 flames, total rewards 32.0\n",
      "Episode finished after 47 flames, total rewards 47.0\n",
      "Episode finished after 34 flames, total rewards 34.0\n",
      "Episode finished after 50 flames, total rewards 50.0\n",
      "Episode finished after 26 flames, total rewards 26.0\n",
      "Episode finished after 34 flames, total rewards 34.0\n",
      "Episode finished after 36 flames, total rewards 36.0\n",
      "Episode finished after 26 flames, total rewards 26.0\n",
      "Episode finished after 35 flames, total rewards 35.0\n",
      "Episode finished after 46 flames, total rewards 46.0\n",
      "Episode finished after 36 flames, total rewards 36.0\n",
      "Episode finished after 51 flames, total rewards 51.0\n"
     ]
    }
   ],
   "source": [
    "#分數有稍微高一點\n",
    "\n",
    "def choose_action(observation):\n",
    "    pos, v, ang, rot = observation\n",
    "    return 0 if ang < 0 else 1 # 柱子左傾則小車左移，否則右移 \n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "#定義完了policy，我們將policy放入我們的程式中\n",
    "for i_episode in range(200):\n",
    "    observation = env.reset()\n",
    "    rewards = 0\n",
    "    for t in range(250):\n",
    "        env.render()\n",
    "        action = choose_action(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        rewards += reward\n",
    "        \n",
    "        #print(\"action:\", action)\n",
    "        #print(\"obs:\", observation)\n",
    "\n",
    "        if done:\n",
    "            print('Episode finished after {} flames, total rewards {}'.format(t+1, rewards))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基於ε-greedy的policy定義\n",
    "\n",
    "def choose_action(state, q_table, action_space, epsilon):\n",
    "    if np.random.random_sample() < epsilon: #有 ε 的機率會選擇隨機 action\n",
    "        return action_space.sample() \n",
    "    else: #其他時間根據現有 policy 選擇 action，也就是在 Q-table 裡目前 state 中，\n",
    "          #選擇擁有最大 Q-value 的 action\n",
    "        return np.argmax(q_table[state]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#環境的feature值都是連續值，不適合做為table的index\n",
    "#需要將一個區間一個區間的值包在一起，用離散值表示(bucket)\n",
    "\n",
    "def get_state(observation, n_buckets, state_bounds):\n",
    "    state = [0] * len(observation) \n",
    "    for i, s in enumerate(observation): # 每個 feature 有不同的分配\n",
    "        l, u = state_bounds[i][0], state_bounds[i][1] # 每個 feature 值的範圍上下限\n",
    "        if s <= l: # 低於下限，分配為 0\n",
    "            state[i] = 0\n",
    "        elif s >= u: # 高於上限，分配為最大值\n",
    "            state[i] = n_buckets[i] - 1\n",
    "        else: # 範圍內，依比例分配\n",
    "            state[i] = int(((s - l) / (u - l)) * n_buckets[i])\n",
    "\n",
    "    return tuple(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 要使用Q-Learning，必須將連續尺寸離散化為多個存儲桶\n",
    "希望有更少的存儲桶，並保持狀態空間盡可能的小。更少的最佳策略來尋找意味著更快的培訓。但是，將狀態空間離散化得太粗糙可能會阻止收斂，因為重要的信息可能會離散化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_table: [[[[[0. 0.]\n",
      "    [0. 0.]\n",
      "    [0. 0.]]\n",
      "\n",
      "   [[0. 0.]\n",
      "    [0. 0.]\n",
      "    [0. 0.]]\n",
      "\n",
      "   [[0. 0.]\n",
      "    [0. 0.]\n",
      "    [0. 0.]]\n",
      "\n",
      "   [[0. 0.]\n",
      "    [0. 0.]\n",
      "    [0. 0.]]\n",
      "\n",
      "   [[0. 0.]\n",
      "    [0. 0.]\n",
      "    [0. 0.]]\n",
      "\n",
      "   [[0. 0.]\n",
      "    [0. 0.]\n",
      "    [0. 0.]]]]]\n",
      "state: (0, 0, 3, 1)\n",
      "q_table的分數: 0.5\n",
      "q_table: [[[[[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [0.  0.5]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]]]]\n",
      "state: (0, 0, 3, 0)\n",
      "q_table的分數: 0.5\n",
      "q_table: [[[[[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0.5]\n",
      "    [0.  0.5]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]]]]\n",
      "state: (0, 0, 2, 0)\n",
      "q_table的分數: 0.5\n",
      "q_table: [[[[[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.5 0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0.5]\n",
      "    [0.  0.5]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [0.  0. ]\n",
      "    [0.  0. ]]]]]\n",
      "state: (0, 0, 2, 0)\n",
      "q_table的分數: 0.25\n",
      "q_table: [[[[[0.   0.  ]\n",
      "    [0.   0.  ]\n",
      "    [0.   0.  ]]\n",
      "\n",
      "   [[0.   0.  ]\n",
      "    [0.   0.  ]\n",
      "    [0.   0.  ]]\n",
      "\n",
      "   [[0.75 0.  ]\n",
      "    [0.   0.  ]\n",
      "    [0.   0.  ]]\n",
      "\n",
      "   [[0.   0.5 ]\n",
      "    [0.   0.5 ]\n",
      "    [0.   0.  ]]\n",
      "\n",
      "   [[0.   0.  ]\n",
      "    [0.   0.  ]\n",
      "    [0.   0.  ]]\n",
      "\n",
      "   [[0.   0.  ]\n",
      "    [0.   0.  ]\n",
      "    [0.   0.  ]]]]]\n",
      "state: (0, 0, 2, 1)\n",
      "q_table的分數: 0.5\n"
     ]
    }
   ],
   "source": [
    "#開始學習，學習到的資料都放在Q table\n",
    "#為了方便收斂，一些參數如ε和learning rate會隨著時間遞減\n",
    "#這表示越來越相信已經學到的經驗了\n",
    "\n",
    "import math\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# 準備 Q-table\n",
    "## Environment中各個 feature 的 bucket 分配數量\n",
    "## 1 代表任何值皆表同一 state，也就是這個 feature 其實不重要\n",
    "n_buckets = (1, 1, 6, 3)\n",
    "\n",
    "## Action 數量 \n",
    "n_actions = env.action_space.n\n",
    "\n",
    "## State 範圍 \n",
    "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "state_bounds[1] = [-0.5, 0.5]\n",
    "state_bounds[3] = [-math.radians(50), math.radians(50)]\n",
    "\n",
    "## Q-table，每個 state-action pair 存一值 \n",
    "q_table = np.zeros(n_buckets + (n_actions,))\n",
    "\n",
    "# 一些學習過程中的參數\n",
    "# epsilon-greedy; 隨時間遞減\n",
    "get_epsilon = lambda i: max(0.01, min(1, 1.0 - math.log10((i+1)/25)))\n",
    "# learning rate; 隨時間遞減 \n",
    "get_lr = lambda i: max(0.01, min(0.5, 1.0 - math.log10((i+1)/25))) \n",
    "gamma = 0.99 # reward discount factor\n",
    "\n",
    "# Q-learning\n",
    "for i_episode in range(1):\n",
    "    epsilon = get_epsilon(i_episode)\n",
    "    lr = get_lr(i_episode)\n",
    "\n",
    "    observation = env.reset()\n",
    "    rewards = 0\n",
    "    state = get_state(observation, n_buckets, state_bounds) # 將連續值轉成離散 \n",
    "    for t in range(5):\n",
    "        action = choose_action(state, q_table, env.action_space, epsilon)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        rewards += reward\n",
    "        next_state = get_state(observation, n_buckets, state_bounds)\n",
    "\n",
    "        # 更新 Q-table\n",
    "        q_next_max = np.amax(q_table[next_state]) # 進入下一個 state 後，預期得到最大總 reward\n",
    "        print(\"q_table:\",q_table)\n",
    "        print(\"state:\",state)\n",
    "        print(\"q_table的分數:\",lr * (reward + gamma * q_next_max - q_table[state + (action,)]))\n",
    "        q_table[state + (action,)] += lr * \\\n",
    "        (reward + gamma * q_next_max - q_table[state + (action,)]) # 就是那個公式\n",
    "\n",
    "        # 前進下一 state \n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print('Episode finished after {} flames, total rewards {}'.format(t+1, rewards))\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到在訓練後期，agent 已經學會如何最大化自己的 reward，也就是維持住小車上的棒子了。\n",
    "\n",
    "這邊有幾個Magic number，包括如何分bucket (哪些feature是重要的)、 state 的上下限、還有參數設定都是需要多試幾次來調整，來達到最好結果的。這邊為了展示給大家看，所以我就放上了前人試過的參數。大家也可以試試看自己從0到1的設定這些參數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
