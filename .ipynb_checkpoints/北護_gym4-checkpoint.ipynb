{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4. 利用Deep Q-Network訓練\n",
    "\n",
    "大家好。歡迎各位再度回來增強式學習的世界\n",
    "我們要利用深度網路來作為Q Function，達到比用Q-Table更好的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 9 timesteps, total rewards 1.9419941711389472\n",
      "Episode finished after 11 timesteps, total rewards 2.5835927210823257\n",
      "Episode finished after 9 timesteps, total rewards 1.482335843574942\n",
      "Episode finished after 9 timesteps, total rewards 2.687485169310189\n",
      "Episode finished after 9 timesteps, total rewards 2.493883846415969\n",
      "Episode finished after 9 timesteps, total rewards 1.8961193465113673\n",
      "Episode finished after 11 timesteps, total rewards 3.2720242916565696\n",
      "Episode finished after 9 timesteps, total rewards 2.364456480107988\n",
      "Episode finished after 9 timesteps, total rewards 2.4771731853576977\n",
      "Episode finished after 9 timesteps, total rewards 0.851510939436956\n",
      "Episode finished after 9 timesteps, total rewards 2.463685634561482\n",
      "Episode finished after 8 timesteps, total rewards 1.0378496893671727\n",
      "Episode finished after 10 timesteps, total rewards 2.678929347345922\n",
      "Episode finished after 9 timesteps, total rewards 1.4393878337492976\n",
      "Episode finished after 8 timesteps, total rewards 0.9828470237012406\n",
      "Episode finished after 10 timesteps, total rewards 2.5839833314574667\n",
      "Episode finished after 8 timesteps, total rewards 1.3979526287857333\n",
      "Episode finished after 10 timesteps, total rewards 2.4005356812746714\n",
      "Episode finished after 9 timesteps, total rewards 0.8061296712824625\n",
      "Episode finished after 11 timesteps, total rewards 2.4835009957378564\n",
      "Episode finished after 10 timesteps, total rewards 1.9604021556239384\n",
      "Episode finished after 11 timesteps, total rewards 3.304586496592512\n",
      "Episode finished after 11 timesteps, total rewards 2.4824951589750484\n",
      "Episode finished after 10 timesteps, total rewards 2.602252426431784\n",
      "Episode finished after 8 timesteps, total rewards 1.43453416287044\n",
      "Episode finished after 10 timesteps, total rewards 2.638069804475747\n",
      "Episode finished after 9 timesteps, total rewards 2.1006246639794046\n",
      "Episode finished after 10 timesteps, total rewards 2.3298157398107997\n",
      "Episode finished after 10 timesteps, total rewards 3.266887279131807\n",
      "Episode finished after 10 timesteps, total rewards 2.7249949271730545\n",
      "Episode finished after 9 timesteps, total rewards 1.4176627372510313\n",
      "Episode finished after 11 timesteps, total rewards 3.128837817305164\n",
      "Episode finished after 10 timesteps, total rewards 3.02924648365348\n",
      "Episode finished after 13 timesteps, total rewards 4.189937348740383\n",
      "Episode finished after 8 timesteps, total rewards 1.3290902974126535\n",
      "Episode finished after 9 timesteps, total rewards 2.5693079032013406\n",
      "Episode finished after 8 timesteps, total rewards 1.2025097383375303\n",
      "Episode finished after 10 timesteps, total rewards 2.9885086999919945\n",
      "Episode finished after 9 timesteps, total rewards 2.337784533915301\n",
      "Episode finished after 10 timesteps, total rewards 2.6127397415549574\n",
      "Episode finished after 9 timesteps, total rewards 2.6146222777833383\n",
      "Episode finished after 9 timesteps, total rewards 1.6134644104468605\n",
      "Episode finished after 10 timesteps, total rewards 1.9460900524372702\n",
      "Episode finished after 9 timesteps, total rewards 2.366541467566339\n",
      "Episode finished after 11 timesteps, total rewards 2.839998253810477\n",
      "Episode finished after 11 timesteps, total rewards 3.198617688185827\n",
      "Episode finished after 9 timesteps, total rewards 1.9142851123266218\n",
      "Episode finished after 10 timesteps, total rewards 2.8497614383150784\n",
      "Episode finished after 9 timesteps, total rewards 0.6663452162030867\n",
      "Episode finished after 11 timesteps, total rewards 2.2733239355136625\n",
      "Episode finished after 10 timesteps, total rewards 3.244621858978961\n",
      "Episode finished after 10 timesteps, total rewards 2.162463954175149\n",
      "Episode finished after 10 timesteps, total rewards 2.6223421153533772\n",
      "Episode finished after 9 timesteps, total rewards 1.6709815192851951\n",
      "Episode finished after 9 timesteps, total rewards 2.2577698900557825\n",
      "Episode finished after 10 timesteps, total rewards 2.767743359876522\n",
      "Episode finished after 11 timesteps, total rewards 3.386615874398421\n",
      "Episode finished after 8 timesteps, total rewards 1.0624616028414584\n",
      "Episode finished after 10 timesteps, total rewards 2.765587811470329\n",
      "Episode finished after 12 timesteps, total rewards 3.9821519898503253\n",
      "Episode finished after 10 timesteps, total rewards 2.9335217553318245\n",
      "Episode finished after 12 timesteps, total rewards 3.858371749448428\n",
      "Episode finished after 12 timesteps, total rewards 4.172195597242937\n",
      "Episode finished after 9 timesteps, total rewards 1.7329811292399335\n",
      "Episode finished after 13 timesteps, total rewards 3.945222471540605\n",
      "Episode finished after 10 timesteps, total rewards 3.0397712998804485\n",
      "Episode finished after 9 timesteps, total rewards 1.8005272256493927\n",
      "Episode finished after 10 timesteps, total rewards 3.0075234354818723\n",
      "Episode finished after 10 timesteps, total rewards 2.106036419870337\n",
      "Episode finished after 9 timesteps, total rewards 1.6439863211911572\n",
      "Episode finished after 8 timesteps, total rewards 0.956525987061377\n",
      "Episode finished after 9 timesteps, total rewards 1.0782396331589408\n",
      "Episode finished after 10 timesteps, total rewards 3.1842694193401755\n",
      "Episode finished after 10 timesteps, total rewards 2.1737783947341596\n",
      "Episode finished after 9 timesteps, total rewards 0.9749139132740416\n",
      "Episode finished after 10 timesteps, total rewards 1.9694538119038605\n",
      "Episode finished after 9 timesteps, total rewards 1.3435798245204185\n",
      "Episode finished after 10 timesteps, total rewards 1.3764172674227342\n",
      "Episode finished after 10 timesteps, total rewards 3.011744714810013\n",
      "Episode finished after 13 timesteps, total rewards 2.102808406077572\n",
      "Episode finished after 10 timesteps, total rewards 3.1550477885223414\n",
      "Episode finished after 10 timesteps, total rewards 3.1033108125854167\n",
      "Episode finished after 10 timesteps, total rewards 2.4879629957584783\n",
      "Episode finished after 9 timesteps, total rewards 1.585265735478628\n",
      "Episode finished after 9 timesteps, total rewards 2.525168969268997\n",
      "Episode finished after 9 timesteps, total rewards 2.0369314536556713\n",
      "Episode finished after 10 timesteps, total rewards 3.0969401519225075\n",
      "Episode finished after 10 timesteps, total rewards 2.444499166880716\n",
      "Episode finished after 9 timesteps, total rewards 2.1163667769610006\n",
      "Episode finished after 10 timesteps, total rewards 2.405397214615682\n",
      "Episode finished after 10 timesteps, total rewards 1.968295965438411\n",
      "Episode finished after 10 timesteps, total rewards 2.4848462728906884\n",
      "Episode finished after 11 timesteps, total rewards 2.801342715336952\n",
      "Episode finished after 9 timesteps, total rewards 1.0556683928269397\n",
      "Episode finished after 12 timesteps, total rewards 3.5278011925606862\n",
      "Episode finished after 10 timesteps, total rewards 2.8982126362985263\n",
      "Episode finished after 10 timesteps, total rewards 3.1561400570121423\n",
      "Episode finished after 11 timesteps, total rewards 2.9272613090164006\n",
      "Episode finished after 9 timesteps, total rewards 1.293795075279316\n",
      "Episode finished after 10 timesteps, total rewards 3.0127562507464756\n",
      "Episode finished after 9 timesteps, total rewards 1.985484319184759\n",
      "Episode finished after 10 timesteps, total rewards 2.951053189608006\n",
      "Episode finished after 9 timesteps, total rewards 2.4141776880532477\n",
      "Episode finished after 11 timesteps, total rewards 3.673697198044149\n",
      "Episode finished after 9 timesteps, total rewards 2.532934057649854\n",
      "Episode finished after 10 timesteps, total rewards 2.2410957609370423\n",
      "Episode finished after 11 timesteps, total rewards 3.1838599845164994\n",
      "Episode finished after 9 timesteps, total rewards 1.3940324182395605\n",
      "Episode finished after 9 timesteps, total rewards 1.2035342572950518\n",
      "Episode finished after 8 timesteps, total rewards 1.2698469487829103\n",
      "Episode finished after 10 timesteps, total rewards 1.906514608949034\n",
      "Episode finished after 11 timesteps, total rewards 2.5081664762769265\n",
      "Episode finished after 11 timesteps, total rewards 2.6559872890929013\n",
      "Episode finished after 9 timesteps, total rewards 1.722852265433287\n",
      "Episode finished after 9 timesteps, total rewards 2.2517329511994935\n",
      "Episode finished after 11 timesteps, total rewards 2.479035104393139\n",
      "Episode finished after 14 timesteps, total rewards 4.516758767669858\n",
      "Episode finished after 10 timesteps, total rewards 2.363574213860988\n",
      "Episode finished after 12 timesteps, total rewards 3.211882741500289\n",
      "Episode finished after 9 timesteps, total rewards 2.2456070123851384\n",
      "Episode finished after 11 timesteps, total rewards 2.312271892731945\n",
      "Episode finished after 9 timesteps, total rewards 2.169321069036601\n",
      "Episode finished after 9 timesteps, total rewards 2.14826056651548\n",
      "Episode finished after 11 timesteps, total rewards 3.546862421101033\n",
      "Episode finished after 9 timesteps, total rewards 1.3424226581670249\n",
      "Episode finished after 9 timesteps, total rewards 2.7811358465997937\n",
      "Episode finished after 9 timesteps, total rewards 0.9154745847799346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 8 timesteps, total rewards 1.520824473193941\n",
      "Episode finished after 10 timesteps, total rewards 1.4364681849368632\n",
      "Episode finished after 10 timesteps, total rewards 2.5430242378530843\n",
      "Episode finished after 8 timesteps, total rewards 1.3213255629983225\n",
      "Episode finished after 9 timesteps, total rewards 1.9564414420496958\n",
      "Episode finished after 10 timesteps, total rewards 2.4125580073767035\n",
      "Episode finished after 9 timesteps, total rewards 2.556135897527449\n",
      "Episode finished after 9 timesteps, total rewards 1.974190605253119\n",
      "Episode finished after 10 timesteps, total rewards 2.3666037065357726\n",
      "Episode finished after 10 timesteps, total rewards 1.688945122504824\n",
      "Episode finished after 10 timesteps, total rewards 2.2856231208706292\n",
      "Episode finished after 10 timesteps, total rewards 3.2093664545711102\n",
      "Episode finished after 10 timesteps, total rewards 3.1565980870741295\n",
      "Episode finished after 9 timesteps, total rewards 2.526962167562107\n",
      "Episode finished after 9 timesteps, total rewards 1.2218358279101005\n",
      "Episode finished after 11 timesteps, total rewards 1.9737132704022757\n",
      "Episode finished after 8 timesteps, total rewards 0.874050289963061\n",
      "Episode finished after 10 timesteps, total rewards 3.108776316379794\n",
      "Episode finished after 10 timesteps, total rewards 2.9313237804229324\n",
      "Episode finished after 10 timesteps, total rewards 2.7476627496008126\n",
      "Episode finished after 11 timesteps, total rewards 3.842695439447944\n",
      "Episode finished after 11 timesteps, total rewards 2.7181848061960796\n",
      "Episode finished after 11 timesteps, total rewards 3.697087934056073\n",
      "Episode finished after 16 timesteps, total rewards 6.8618137660991865\n",
      "Episode finished after 10 timesteps, total rewards 2.2777094392401374\n",
      "Episode finished after 12 timesteps, total rewards 3.4962900737870233\n",
      "Episode finished after 9 timesteps, total rewards 2.377934522335586\n",
      "Episode finished after 9 timesteps, total rewards 2.2642133220473486\n",
      "Episode finished after 9 timesteps, total rewards 1.015105847665934\n",
      "Episode finished after 9 timesteps, total rewards 1.334066446131617\n",
      "Episode finished after 9 timesteps, total rewards 2.0009510541901676\n",
      "Episode finished after 10 timesteps, total rewards 3.260343986187921\n",
      "Episode finished after 8 timesteps, total rewards 1.4277788624363523\n",
      "Episode finished after 10 timesteps, total rewards 1.6261354349557344\n",
      "Episode finished after 9 timesteps, total rewards 2.6622857062813012\n",
      "Episode finished after 12 timesteps, total rewards 3.252307239314784\n",
      "Episode finished after 9 timesteps, total rewards 1.4225212361052755\n",
      "Episode finished after 12 timesteps, total rewards 4.142578650431274\n",
      "Episode finished after 9 timesteps, total rewards 2.8754551913070454\n",
      "Episode finished after 11 timesteps, total rewards 2.4051219575947056\n",
      "Episode finished after 9 timesteps, total rewards 2.2442371353154256\n",
      "Episode finished after 10 timesteps, total rewards 3.155743933589015\n",
      "Episode finished after 12 timesteps, total rewards 2.9775637166685294\n",
      "Episode finished after 9 timesteps, total rewards 1.9655268599072095\n",
      "Episode finished after 10 timesteps, total rewards 2.191075639735687\n",
      "Episode finished after 10 timesteps, total rewards 2.704789727758553\n",
      "Episode finished after 9 timesteps, total rewards 1.7170924065175481\n",
      "Episode finished after 13 timesteps, total rewards 4.027200990106433\n",
      "Episode finished after 12 timesteps, total rewards 4.163897520792833\n",
      "Episode finished after 10 timesteps, total rewards 3.2985224013267476\n",
      "Episode finished after 9 timesteps, total rewards 1.4243719309153249\n",
      "Episode finished after 12 timesteps, total rewards 3.3008148974783813\n",
      "Episode finished after 12 timesteps, total rewards 3.8509342598794967\n",
      "Episode finished after 10 timesteps, total rewards 2.656732474634847\n",
      "Episode finished after 11 timesteps, total rewards 2.9631845998320707\n",
      "Episode finished after 10 timesteps, total rewards 2.38101327767419\n",
      "Episode finished after 10 timesteps, total rewards 1.5270344225240917\n",
      "Episode finished after 9 timesteps, total rewards 1.3759450708244096\n",
      "Episode finished after 9 timesteps, total rewards 1.4419624353775953\n",
      "Episode finished after 10 timesteps, total rewards 2.9914465143199056\n",
      "Episode finished after 9 timesteps, total rewards 1.128925251519505\n",
      "Episode finished after 9 timesteps, total rewards 2.4403844463617537\n",
      "Episode finished after 9 timesteps, total rewards 1.763434758672787\n",
      "Episode finished after 10 timesteps, total rewards 2.413759689948504\n",
      "Episode finished after 9 timesteps, total rewards 2.467660666675104\n",
      "Episode finished after 10 timesteps, total rewards 3.0935292902887292\n",
      "Episode finished after 10 timesteps, total rewards 3.178422626795162\n",
      "Episode finished after 10 timesteps, total rewards 2.2122463884758266\n",
      "Episode finished after 10 timesteps, total rewards 2.8482842958369234\n",
      "Episode finished after 11 timesteps, total rewards 2.630001112257921\n",
      "Episode finished after 10 timesteps, total rewards 2.3412751568545804\n",
      "Episode finished after 11 timesteps, total rewards 3.1866997479076664\n",
      "Episode finished after 10 timesteps, total rewards 2.609341061415116\n",
      "Episode finished after 10 timesteps, total rewards 3.109702571973071\n",
      "Episode finished after 11 timesteps, total rewards 3.191852110052806\n",
      "Episode finished after 12 timesteps, total rewards 2.345904264131981\n",
      "Episode finished after 9 timesteps, total rewards 1.5593905202393965\n",
      "Episode finished after 9 timesteps, total rewards 1.5705365769646118\n",
      "Episode finished after 14 timesteps, total rewards 3.3627035297054184\n",
      "Episode finished after 9 timesteps, total rewards 2.199007796004805\n",
      "Episode finished after 10 timesteps, total rewards 2.4874660476119153\n",
      "Episode finished after 9 timesteps, total rewards 1.440325504886737\n",
      "Episode finished after 9 timesteps, total rewards 2.0620219287826957\n",
      "Episode finished after 11 timesteps, total rewards 0.9477418005403777\n",
      "Episode finished after 10 timesteps, total rewards 1.5410651110220888\n",
      "Episode finished after 10 timesteps, total rewards 2.8280973197710417\n",
      "Episode finished after 11 timesteps, total rewards 2.42479807322085\n",
      "Episode finished after 70 timesteps, total rewards 10.389335665244207\n",
      "Episode finished after 24 timesteps, total rewards 6.330758285964646\n",
      "Episode finished after 12 timesteps, total rewards 3.721162430651458\n",
      "Episode finished after 25 timesteps, total rewards 3.0311124448485227\n",
      "Episode finished after 56 timesteps, total rewards 7.718724829209757\n",
      "Episode finished after 49 timesteps, total rewards 2.8757982356935448\n",
      "Episode finished after 65 timesteps, total rewards 16.099006934743326\n",
      "Episode finished after 59 timesteps, total rewards 17.1462568064777\n",
      "Episode finished after 82 timesteps, total rewards 25.54938759088898\n",
      "Episode finished after 64 timesteps, total rewards 13.377873301812183\n",
      "Episode finished after 79 timesteps, total rewards 28.520864603814974\n",
      "Episode finished after 70 timesteps, total rewards 18.208381487785832\n",
      "Episode finished after 113 timesteps, total rewards 41.63022039739759\n",
      "Episode finished after 198 timesteps, total rewards 39.94409406768717\n",
      "Episode finished after 130 timesteps, total rewards 44.70913439798829\n",
      "Episode finished after 424 timesteps, total rewards 184.12884726167474\n",
      "Episode finished after 280 timesteps, total rewards 84.33323958134595\n",
      "Episode finished after 380 timesteps, total rewards 127.8365332270649\n",
      "Episode finished after 594 timesteps, total rewards 245.4037586265923\n",
      "Episode finished after 406 timesteps, total rewards 116.9621826790174\n",
      "Episode finished after 1456 timesteps, total rewards 295.73668966074507\n",
      "Episode finished after 6490 timesteps, total rewards 2957.8970229659553\n",
      "Episode finished after 723 timesteps, total rewards 288.50328577053267\n",
      "Episode finished after 315 timesteps, total rewards 102.07643173776889\n",
      "Episode finished after 1596 timesteps, total rewards 674.234773467011\n",
      "Episode finished after 608 timesteps, total rewards 183.08591457157405\n",
      "Episode finished after 1001 timesteps, total rewards 341.51289158928375\n",
      "Episode finished after 2302 timesteps, total rewards 787.483904977342\n",
      "Episode finished after 1444 timesteps, total rewards 471.47181019358317\n",
      "Episode finished after 1544 timesteps, total rewards 518.729761490593\n",
      "Episode finished after 641 timesteps, total rewards 190.96645921542458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 782 timesteps, total rewards 162.67979440305524\n",
      "Episode finished after 2972 timesteps, total rewards 1464.2823172597798\n",
      "Episode finished after 1057 timesteps, total rewards 351.49546139485966\n",
      "Episode finished after 1012 timesteps, total rewards 299.0294929957362\n",
      "Episode finished after 1060 timesteps, total rewards 127.13701342844016\n",
      "Episode finished after 934 timesteps, total rewards 350.3905045979366\n",
      "Episode finished after 1085 timesteps, total rewards 227.75107045908314\n",
      "Episode finished after 1554 timesteps, total rewards 562.6292338019906\n",
      "Episode finished after 1860 timesteps, total rewards 328.39429274378597\n",
      "Episode finished after 1553 timesteps, total rewards 607.6517513755065\n",
      "Episode finished after 459 timesteps, total rewards 152.2203103565231\n",
      "Episode finished after 1368 timesteps, total rewards 458.30913143135587\n",
      "Episode finished after 1443 timesteps, total rewards 396.0678795105274\n",
      "Episode finished after 1014 timesteps, total rewards 439.147793209998\n",
      "Episode finished after 631 timesteps, total rewards 121.59096826520862\n",
      "Episode finished after 883 timesteps, total rewards 338.405717590285\n",
      "Episode finished after 1422 timesteps, total rewards 646.8793730701192\n",
      "Episode finished after 1362 timesteps, total rewards 607.2098796827227\n",
      "Episode finished after 865 timesteps, total rewards 255.34719445955398\n",
      "Episode finished after 1363 timesteps, total rewards 374.87300036595394\n",
      "Episode finished after 1135 timesteps, total rewards 468.2094037686189\n",
      "Episode finished after 178 timesteps, total rewards 62.43677053562067\n",
      "Episode finished after 1064 timesteps, total rewards 178.03398976652292\n",
      "Episode finished after 891 timesteps, total rewards 386.05391810697137\n",
      "Episode finished after 605 timesteps, total rewards 202.1605523781298\n",
      "Episode finished after 900 timesteps, total rewards 272.0771607809535\n",
      "Episode finished after 815 timesteps, total rewards 243.5889109776431\n",
      "Episode finished after 488 timesteps, total rewards 109.99565524427561\n",
      "Episode finished after 1071 timesteps, total rewards 400.9412289903996\n",
      "Episode finished after 2496 timesteps, total rewards 912.9682808302292\n",
      "Episode finished after 1007 timesteps, total rewards 334.0627149217747\n",
      "Episode finished after 2500 timesteps, total rewards 688.3538903044486\n",
      "Episode finished after 1690 timesteps, total rewards 792.2382656651752\n",
      "Episode finished after 834 timesteps, total rewards 164.00972844688977\n",
      "Episode finished after 1205 timesteps, total rewards 557.5308352135729\n",
      "Episode finished after 2282 timesteps, total rewards 849.0879783096502\n",
      "Episode finished after 1425 timesteps, total rewards 470.47482918294344\n",
      "Episode finished after 2503 timesteps, total rewards 907.8923459660571\n",
      "Episode finished after 746 timesteps, total rewards 230.51434794511465\n",
      "Episode finished after 1123 timesteps, total rewards 283.59955639864324\n",
      "Episode finished after 1414 timesteps, total rewards 395.1743748933026\n",
      "Episode finished after 3070 timesteps, total rewards 944.8468115434152\n",
      "Episode finished after 2100 timesteps, total rewards 954.9126541142296\n",
      "Episode finished after 1689 timesteps, total rewards 738.0856977846373\n",
      "Episode finished after 1573 timesteps, total rewards 616.0755970573255\n",
      "Episode finished after 1139 timesteps, total rewards 455.53127073012615\n",
      "Episode finished after 1483 timesteps, total rewards 469.60243565745964\n",
      "Episode finished after 797 timesteps, total rewards 246.16670082035603\n",
      "Episode finished after 1449 timesteps, total rewards 365.3198739222233\n",
      "Episode finished after 1043 timesteps, total rewards 379.1200270490753\n",
      "Episode finished after 709 timesteps, total rewards 232.17300003922583\n",
      "Episode finished after 756 timesteps, total rewards 193.52262948472054\n",
      "Episode finished after 590 timesteps, total rewards 189.7392073069738\n",
      "Episode finished after 1653 timesteps, total rewards 687.0967748885543\n",
      "Episode finished after 771 timesteps, total rewards 258.64100190990877\n",
      "Episode finished after 765 timesteps, total rewards 278.6512603709416\n",
      "Episode finished after 786 timesteps, total rewards 280.02127458392016\n",
      "Episode finished after 6832 timesteps, total rewards 1444.9167868240793\n",
      "Episode finished after 1172 timesteps, total rewards 298.8098172257634\n",
      "Episode finished after 2453 timesteps, total rewards 793.5426984829817\n",
      "Episode finished after 2114 timesteps, total rewards 375.4142063932797\n",
      "Episode finished after 2024 timesteps, total rewards 839.1560578386054\n",
      "Episode finished after 1532 timesteps, total rewards 686.2659303055925\n",
      "Episode finished after 842 timesteps, total rewards 306.9165336018492\n",
      "Episode finished after 1333 timesteps, total rewards 436.6051021208758\n",
      "Episode finished after 2134 timesteps, total rewards 518.4354200044764\n",
      "Episode finished after 440 timesteps, total rewards 116.22363119984091\n",
      "Episode finished after 1686 timesteps, total rewards 408.66565540443304\n",
      "Episode finished after 870 timesteps, total rewards 344.01989490643183\n",
      "Episode finished after 1458 timesteps, total rewards 709.694918756986\n",
      "Episode finished after 625 timesteps, total rewards 194.1465176107083\n",
      "Episode finished after 1594 timesteps, total rewards 555.8095502905229\n",
      "Episode finished after 937 timesteps, total rewards 168.17780789506543\n",
      "Episode finished after 910 timesteps, total rewards 347.7600673062516\n",
      "Episode finished after 851 timesteps, total rewards 303.383989198036\n",
      "Episode finished after 1565 timesteps, total rewards 556.5453572829357\n",
      "Episode finished after 802 timesteps, total rewards 167.58011014354352\n",
      "Episode finished after 930 timesteps, total rewards 195.908511981366\n",
      "Episode finished after 967 timesteps, total rewards 314.7192278990398\n",
      "Episode finished after 647 timesteps, total rewards 148.48515352463133\n",
      "Episode finished after 721 timesteps, total rewards 222.4872809853908\n",
      "Episode finished after 931 timesteps, total rewards 305.8413487567295\n",
      "Episode finished after 983 timesteps, total rewards 479.0404224014518\n",
      "Episode finished after 850 timesteps, total rewards 397.90349820582054\n",
      "Episode finished after 421 timesteps, total rewards 96.23482900858156\n",
      "Episode finished after 1479 timesteps, total rewards 427.71164649861447\n",
      "Episode finished after 446 timesteps, total rewards 128.53679887372513\n",
      "Episode finished after 823 timesteps, total rewards 386.9180801693931\n",
      "Episode finished after 766 timesteps, total rewards 341.3472321842631\n",
      "Episode finished after 961 timesteps, total rewards 287.15982549882904\n",
      "Episode finished after 770 timesteps, total rewards 173.60889345160746\n",
      "Episode finished after 719 timesteps, total rewards 249.73755928059933\n",
      "Episode finished after 783 timesteps, total rewards 301.4702704048588\n",
      "Episode finished after 1431 timesteps, total rewards 635.4346553476955\n",
      "Episode finished after 1066 timesteps, total rewards 405.5862526860567\n",
      "Episode finished after 665 timesteps, total rewards 159.66715853752882\n",
      "Episode finished after 749 timesteps, total rewards 210.21577948272486\n",
      "Episode finished after 1162 timesteps, total rewards 559.4118858253462\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "\n",
    "# Cheating mode speeds up the training process\n",
    "CHEAT = True\n",
    "\n",
    "\n",
    "#建立一層隱藏層，將state傳入後，得出每個action分數\n",
    "#分數越高的action，越有機會被選中\n",
    "#我們的目標是在當前state下，對未來越有利的action分數越高\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, n_hidden):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        #輸入層(state)到隱藏層，隱藏層到輸出層(action)\n",
    "        self.fc1 = nn.Linear(n_states, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "\n",
    "\n",
    "#Deep Q-Network是由兩個network組成的，evaluation network(eval_net)與target network(target_net)\n",
    "#還需要memory儲存experience與設定好的參數\n",
    "class DQN(object):\n",
    "    def __init__(self, n_states, n_actions, n_hidden, batch_size, lr, epsilon, gamma, target_replace_iter, memory_capacity):\n",
    "        self.eval_net, self.target_net = Net(n_states, n_actions, n_hidden), Net(n_states, n_actions, n_hidden)\n",
    "\n",
    "        self.memory = np.zeros((memory_capacity, n_states * 2 + 2)) # initialize memory, each memory slot is of size (state + next state + reward + action)\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=lr)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.memory_counter = 0\n",
    "        self.learn_step_counter = 0 # for target network update\n",
    "\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.target_replace_iter = target_replace_iter\n",
    "        self.memory_capacity = memory_capacity\n",
    "\n",
    "    #會根據ε-greedy policy選擇action\n",
    "    #ε會讓action隨機亂走，如此才有機會可以學到新經驗\n",
    "    def choose_action(self, state):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(state), 0)\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if np.random.uniform() < self.epsilon: # random\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        else: # greedy\n",
    "            actions_value = self.eval_net(x) # feed into eval net, get scores for each action\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()[0] # choose the one with the largest score\n",
    "\n",
    "        return action\n",
    "    \n",
    "    #儲存experience\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        # Pack the experience\n",
    "        transition = np.hstack((state, [action, reward], next_state))\n",
    "\n",
    "        # Replace the old memory with new memory\n",
    "        index = self.memory_counter % self.memory_capacity\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    #從memory中取樣學習\n",
    "    def learn(self):\n",
    "        # Randomly select a batch of memory to learn from\n",
    "        sample_index = np.random.choice(self.memory_capacity, self.batch_size)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_state = torch.FloatTensor(b_memory[:, :self.n_states])\n",
    "        b_action = torch.LongTensor(b_memory[:, self.n_states:self.n_states+1].astype(int))\n",
    "        b_reward = torch.FloatTensor(b_memory[:, self.n_states+1:self.n_states+2])\n",
    "        b_next_state = torch.FloatTensor(b_memory[:, -self.n_states:])\n",
    "\n",
    "        #計算現有 eval net 和 target net 得出 Q value 的落差\n",
    "        q_eval = self.eval_net(b_state).gather(1, b_action) # evaluate the Q values of the experiences, given the states & actions taken at that time\n",
    "        q_next = self.target_net(b_next_state).detach() # detach from graph, don't backpropagate\n",
    "        q_target = b_reward + self.gamma * q_next.max(1)[0].view(self.batch_size, 1) # compute the target Q values\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #每隔一段時間 (target_replace_iter), 更新 target net，即複製 eval net 到 target net\n",
    "        self.learn_step_counter += 1\n",
    "        if self.learn_step_counter % self.target_replace_iter == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('CartPole-v0')\n",
    "    env = env.unwrapped # For cheating mode to access values hidden in the environment\n",
    "\n",
    "    # Environment parameters\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = env.observation_space.shape[0]\n",
    "\n",
    "    # Hyper parameters\n",
    "    n_hidden = 50\n",
    "    batch_size = 32\n",
    "    lr = 0.01                 # learning rate\n",
    "    epsilon = 0.1             # epsilon-greedy, factor to explore randomly\n",
    "    gamma = 0.9               # reward discount factor\n",
    "    target_replace_iter = 100 # target network update frequency\n",
    "    memory_capacity = 2000\n",
    "    n_episodes = 400 if CHEAT else 4000\n",
    "\n",
    "    # Create DQN\n",
    "    dqn = DQN(n_states, n_actions, n_hidden, batch_size, lr, epsilon, gamma, target_replace_iter, memory_capacity)\n",
    "\n",
    "    #學習\n",
    "    for i_episode in range(n_episodes):\n",
    "        t = 0 # timestep\n",
    "        rewards = 0 # accumulate rewards for each episode\n",
    "        state = env.reset() # reset environment to initial state for each episode\n",
    "        while True:\n",
    "            env.render()\n",
    "\n",
    "            # Agent takes action\n",
    "            action = dqn.choose_action(state) # choose an action based on DQN\n",
    "            next_state, reward, done, info = env.step(action) # do the action, get the reward\n",
    "\n",
    "            # Cheating part: modify the reward to speed up training process\n",
    "            if CHEAT:\n",
    "                x, v, theta, omega = next_state\n",
    "                r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8 # reward 1: the closer the cart is to the center, the better\n",
    "                r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5 # reward 2: the closer the pole is to the center, the better\n",
    "                reward = r1 + r2\n",
    "\n",
    "            # Keep the experience in memory\n",
    "            dqn.store_transition(state, action, reward, next_state)\n",
    "\n",
    "            # Accumulate reward\n",
    "            rewards += reward\n",
    "\n",
    "            #有足夠 experience 後進行訓練\n",
    "            if dqn.memory_counter > memory_capacity:\n",
    "                dqn.learn()\n",
    "\n",
    "            # 進入下一 state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Episode finished after {} timesteps, total rewards {}'.format(t+1, rewards))\n",
    "                break\n",
    "\n",
    "            t += 1\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
