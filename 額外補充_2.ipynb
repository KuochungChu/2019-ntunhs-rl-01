{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains a DQN/DDQN to solve CartPole-v0 problem\n",
    "\"\"\"\n",
    "\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, state_space, action_space, args, episodes=500):\n",
    "\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # experience buffer\n",
    "        self.memory = []\n",
    "\n",
    "        # discount rate\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        # initially 90% exploration, 10% exploitation\n",
    "        self.epsilon = 1.0\n",
    "        # iteratively applying decay til 10% exploration/90% exploitation\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = self.epsilon_min / self.epsilon\n",
    "        self.epsilon_decay = self.epsilon_decay ** (1. / float(episodes))\n",
    "\n",
    "        # Q Network weights filename\n",
    "        self.weights_file = 'dqn_cartpole.h5'\n",
    "        # Q Network for training\n",
    "        n_inputs = state_space.shape[0]\n",
    "        n_outputs = action_space.n\n",
    "        self.q_model = self.build_model(n_inputs, n_outputs)\n",
    "        self.q_model.compile(loss='mse', optimizer=Adam())\n",
    "        # target Q Network\n",
    "        self.target_q_model = self.build_model(n_inputs, n_outputs)\n",
    "        # copy Q Network params to target Q Network\n",
    "        self.update_weights()\n",
    "\n",
    "        self.replay_counter = 0\n",
    "        self.ddqn = True if args.ddqn else False\n",
    "        if self.ddqn:\n",
    "            print(\"----------Double DQN--------\")\n",
    "        else:\n",
    "            print(\"-------------DQN------------\")\n",
    "\n",
    "    \n",
    "    # Q Network is 256-256-256 MLP\n",
    "    def build_model(self, n_inputs, n_outputs):\n",
    "        inputs = Input(shape=(n_inputs, ), name='state')\n",
    "        x = Dense(256, activation='relu')(inputs)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(n_outputs, activation='linear', name='action')(x)\n",
    "        q_model = Model(inputs, x)\n",
    "        q_model.summary()\n",
    "        return q_model\n",
    "\n",
    "\n",
    "    # save Q Network params to a file\n",
    "    def save_weights(self):\n",
    "        self.q_model.save_weights(self.weights_file)\n",
    "\n",
    "\n",
    "    # copy trained Q Network params to target Q Network\n",
    "    def update_weights(self):\n",
    "        self.target_q_model.set_weights(self.q_model.get_weights())\n",
    "\n",
    "\n",
    "    # eps-greedy policy\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # explore - do random action\n",
    "            return self.action_space.sample()\n",
    "\n",
    "        # exploit\n",
    "        q_values = self.q_model.predict(state)\n",
    "        # select the action with max Q-value\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "\n",
    "    # store experiences in the replay buffer\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        self.memory.append(item)\n",
    "\n",
    "\n",
    "    # compute Q_max\n",
    "    # use of target Q Network solves the non-stationarity problem\n",
    "    def get_target_q_value(self, next_state, reward):\n",
    "        # max Q value among next state's actions\n",
    "        if self.ddqn:\n",
    "            # DDQN\n",
    "            # current Q Network selects the action\n",
    "            # a'_max = argmax_a' Q(s', a')\n",
    "            action = np.argmax(self.q_model.predict(next_state)[0])\n",
    "            # target Q Network evaluates the action\n",
    "            # Q_max = Q_target(s', a'_max)\n",
    "            q_value = self.target_q_model.predict(next_state)[0][action]\n",
    "        else:\n",
    "            # DQN chooses the max Q value among next actions\n",
    "            # selection and evaluation of action is on the target Q Network\n",
    "            # Q_max = max_a' Q_target(s', a')\n",
    "            q_value = np.amax(self.target_q_model.predict(next_state)[0])\n",
    "\n",
    "        # Q_max = reward + gamma * Q_max\n",
    "        q_value *= self.gamma\n",
    "        q_value += reward\n",
    "        return q_value\n",
    "\n",
    "\n",
    "    # experience replay addresses the correlation issue between samples\n",
    "    def replay(self, batch_size):\n",
    "        # sars = state, action, reward, state' (next_state)\n",
    "        sars_batch = random.sample(self.memory, batch_size)\n",
    "        state_batch, q_values_batch = [], []\n",
    "\n",
    "        # fixme: for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for state, action, reward, next_state, done in sars_batch:\n",
    "            # policy prediction for a given state\n",
    "            q_values = self.q_model.predict(state)\n",
    "\n",
    "            # get Q_max\n",
    "            q_value = self.get_target_q_value(next_state, reward)\n",
    "\n",
    "            # correction on the Q value for the action used\n",
    "            q_values[0][action] = reward if done else q_value\n",
    "\n",
    "            # collect batch state-q_value mapping\n",
    "            state_batch.append(state[0])\n",
    "            q_values_batch.append(q_values[0])\n",
    "\n",
    "        # train the Q-network\n",
    "        self.q_model.fit(np.array(state_batch),\n",
    "                         np.array(q_values_batch),\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=1,\n",
    "                         verbose=0)\n",
    "\n",
    "        # update exploration-exploitation probability\n",
    "        self.update_epsilon()\n",
    "\n",
    "        # copy new params on old target after every 10 training updates\n",
    "        if self.replay_counter % 10 == 0:\n",
    "            self.update_weights()\n",
    "\n",
    "        self.replay_counter += 1\n",
    "\n",
    "    \n",
    "    # decrease the exploration, increase exploitation\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument('env_id',\n",
    "                        nargs='?',\n",
    "                        default='CartPole-v0',\n",
    "                        help='Select the environment to run')\n",
    "    parser.add_argument(\"-d\",\n",
    "                        \"--ddqn\",\n",
    "                        action='store_true',\n",
    "                        help=\"Use Double DQN\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # the number of trials without falling over\n",
    "    win_trials = 100\n",
    "\n",
    "    # the CartPole-v0 is considered solved if for 100 consecutive trials,\n",
    "    # the cart pole has not fallen over and it has achieved an average \n",
    "    # reward of 195.0\n",
    "    # a reward of +1 is provided for every timestep the pole remains\n",
    "    # upright\n",
    "    win_reward = { 'CartPole-v0' : 195.0 }\n",
    "\n",
    "    # stores the reward per episode\n",
    "    scores = deque(maxlen=win_trials)\n",
    "\n",
    "    logger.setLevel(logger.ERROR)\n",
    "    env = gym.make(args.env_id)\n",
    "\n",
    "    outdir = \"/tmp/dqn-%s\" % args.env_id\n",
    "    if args.ddqn:\n",
    "        outdir = \"/tmp/ddqn-%s\" % args.env_id\n",
    "\n",
    "    env = wrappers.Monitor(env, directory=outdir, force=True)\n",
    "    env.seed(0)\n",
    "\n",
    "    # instantiate the DQN/DDQN agent\n",
    "    agent = DQNAgent(env.observation_space, env.action_space, args)\n",
    "\n",
    "    # should be solved in this number of episodes\n",
    "    episode_count = 3000\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    batch_size = 64\n",
    "\n",
    "    # by default, CartPole-v0 has max episode steps = 200\n",
    "    # you can use this to experiment beyond 200\n",
    "    # env._max_episode_steps = 4000\n",
    "\n",
    "    # Q-Learning sampling and fitting\n",
    "    for episode in range(episode_count):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            # in CartPole-v0, action=0 is left and action=1 is right\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # in CartPole-v0:\n",
    "            # state = [pos, vel, theta, angular speed]\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # store every experience unit in replay buffer\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "\n",
    "        # call experience relay\n",
    "        if len(agent.memory) >= batch_size:\n",
    "            agent.replay(batch_size)\n",
    "    \n",
    "        scores.append(total_reward)\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score >= win_reward[args.env_id] and episode >= win_trials:\n",
    "            print(\"Solved in episode %d: Mean survival = %0.2lf in %d episodes\"\n",
    "                  % (episode, mean_score, win_trials))\n",
    "            print(\"Epsilon: \", agent.epsilon)\n",
    "            agent.save_weights()\n",
    "            break\n",
    "        if episode % win_trials == 0:\n",
    "            print(\"Episode %d: Mean survival = %0.2lf in %d episodes\" %\n",
    "                  (episode, mean_score, win_trials))\n",
    "\n",
    "    # close the env and write monitor result info to disk\n",
    "    env.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
